🚧 Project Highlight — Claim Approval Prediction using Synthetic Data

I explored a multiclass classification problem to predict healthcare claim statuses (Approved, Denied, Pending) using a synthetic dataset generated by Faker (via Kaggle).

✅ Steps I followed:
- Cleaned and encoded mixed data types
- Built pipelines using RandomForest, SVC, and XGBoost with SelectFromModel
- Used train-test split and cross-validation

❗ Key Finding:
 Despite trying multiple models and tuning, the highest accuracy hovered around ~36%.

📉 Why so low?
1) The dataset was fully synthetic, generated without real-world relationships between features and targets.
2) This project reminded me that good models require good data. Without real patterns, even the best ML pipelines can’t do much.

💡 Realizing this, I shifted to unsupervised learning, thinking that uncorrelated data may still hold natural clusters. I applied PCA to reduce dimensionality and used Clustering to group the data into 3 clusters 'hoping to uncover hidden structure'.
📉 However, the cluster compositions were nearly identical in terms of claim status distribution. Each cluster had an almost equal mix of Approved, Denied, and Pending claims — suggesting no significant separation exists even in unsupervised space.

🔎 Key Takeaway:
 This project reinforced an important lesson — not all datasets contain patterns worth modeling, and understanding when a model doesn’t work is just as valuable as when it does.
